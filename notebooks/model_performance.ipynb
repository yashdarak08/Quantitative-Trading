{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "This notebook evaluates the performance of the forecasting models and analyzes their prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Add src directory to path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import our modules\n",
    "from src.data_loader import fetch_data, prepare_dataset\n",
    "from src.model import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import yaml\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Fetch data\n",
    "ticker = config['data']['tickers'][1]  # S&P500\n",
    "start_date = config['data']['start_date']\n",
    "end_date = config['data']['end_date']\n",
    "\n",
    "print(f\"Fetching data for {ticker} from {start_date} to {end_date}\")\n",
    "data = fetch_data(ticker, start_date, end_date)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset with a sliding window\n",
    "window_size = 60\n",
    "X, y = prepare_dataset(data, window_size)\n",
    "\n",
    "# Split dataset into training and testing sets (80/20 split)\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train LSTM model\n",
    "model = build_model(\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    lstm_units=config['model']['lstm_units'],\n",
    "    dropout_rate=config['model']['dropout'],\n",
    "    model_type='LSTM'\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=config['model']['epochs'],\n",
    "    batch_size=config['model']['batch_size'],\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "y_true = y_test\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "corr, _ = pearsonr(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (RÂ²): {r2:.4f}\")\n",
    "print(f\"Correlation Coefficient: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual and predicted values\n",
    "test_data = data.iloc[split_index + window_size:].copy()\n",
    "test_data = test_data.iloc[:len(y_pred)]\n",
    "test_data['Predicted'] = y_pred\n",
    "\n",
    "# Plot actual vs predicted prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_data.index, test_data['Price'], label='Actual Price')\n",
    "plt.plot(test_data.index, test_data['Predicted'], label='Predicted Price', alpha=0.7)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Actual vs Predicted Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction errors\n",
    "test_data['Error'] = test_data['Price'] - test_data['Predicted']\n",
    "test_data['Percent_Error'] = (test_data['Error'] / test_data['Price']) * 100\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Error over time\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(test_data.index, test_data['Error'])\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Prediction Error Over Time')\n",
    "plt.grid(True)\n",
    "\n",
    "# Error histogram\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.histplot(test_data['Percent_Error'], kde=True)\n",
    "plt.xlabel('Percent Error (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Error Distribution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price changes\n",
    "test_data['Actual_Change'] = test_data['Price'].diff()\n",
    "test_data['Predicted_Change'] = test_data['Predicted'].diff()\n",
    "\n",
    "# Determine if direction prediction was correct\n",
    "test_data['Direction_Match'] = (test_data['Actual_Change'] * test_data['Predicted_Change']) > 0\n",
    "\n",
    "# Calculate accuracy\n",
    "direction_accuracy = test_data['Direction_Match'].mean()\n",
    "print(f\"Direction Prediction Accuracy: {direction_accuracy:.4f} ({direction_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Confusion matrix for direction prediction\n",
    "actual_up = test_data['Actual_Change'] > 0\n",
    "predicted_up = test_data['Predicted_Change'] > 0\n",
    "\n",
    "true_up = (actual_up & predicted_up).sum()\n",
    "false_up = (~actual_up & predicted_up).sum()\n",
    "true_down = (~actual_up & ~predicted_up).sum()\n",
    "false_down = (actual_up & ~predicted_up).sum()\n",
    "\n",
    "print(\"\\nDirection Prediction Confusion Matrix:\")\n",
    "print(f\"True Up: {true_up}, False Up: {false_up}\")\n",
    "print(f\"True Down: {true_down}, False Down: {false_down}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "model_path = '../models/lstm_model'\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
