{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Exploratory Data Analysis\n",
       "\n",
       "This notebook performs exploratory data analysis on the financial data used for forecasting and strategy development. We analyze various statistical properties, seasonality, autocorrelation, and other characteristics of the price series to inform our strategy design."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "import yaml\n",
       "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
       "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
       "import scipy.stats as stats\n",
       "\n",
       "# Add src directory to path\n",
       "import sys\n",
       "sys.path.append('../')\n",
       "\n",
       "# Import our data loading module\n",
       "from src.data_loader import fetch_data\n",
       "\n",
       "# Set up plotting style\n",
       "plt.style.use('ggplot')\n",
       "sns.set_style('whitegrid')\n",
       "%matplotlib inline"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Load Configuration & Data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load configuration\n",
       "with open('../config/config.yaml', 'r') as f:\n",
       "    config = yaml.safe_load(f)\n",
       "\n",
       "# Get tickers from config\n",
       "tickers = config['data']['tickers']\n",
       "start_date = config['data']['start_date']\n",
       "end_date = config['data']['end_date']\n",
       "\n",
       "print(f\"Analyzing data for tickers: {tickers}\")\n",
       "print(f\"Period: {start_date} to {end_date}\")\n",
       "\n",
       "# Fetch data for each ticker\n",
       "data_dict = {}\n",
       "for ticker in tickers:\n",
       "    data_dict[ticker] = fetch_data(ticker, start_date, end_date)\n",
       "    print(f\"Loaded {ticker}: {len(data_dict[ticker])} rows of data\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Basic Price Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot price series for each ticker\n",
       "plt.figure(figsize=(14, 7 * len(tickers)))\n",
       "\n",
       "for i, ticker in enumerate(tickers, 1):\n",
       "    plt.subplot(len(tickers), 1, i)\n",
       "    data_dict[ticker]['Price'].plot()\n",
       "    plt.title(f'{ticker} Price Series')\n",
       "    plt.xlabel('Date')\n",
       "    plt.ylabel('Price')\n",
       "    plt.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Return Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate daily returns\n",
       "returns_dict = {}\n",
       "for ticker in tickers:\n",
       "    returns_dict[ticker] = data_dict[ticker]['Price'].pct_change().dropna()\n",
       "    \n",
       "# Plot daily returns\n",
       "plt.figure(figsize=(14, 7 * len(tickers)))\n",
       "\n",
       "for i, ticker in enumerate(tickers, 1):\n",
       "    plt.subplot(len(tickers), 1, i)\n",
       "    returns_dict[ticker].plot()\n",
       "    plt.title(f'{ticker} Daily Returns')\n",
       "    plt.xlabel('Date')\n",
       "    plt.ylabel('Return')\n",
       "    plt.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Statistical Properties"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate and display key statistics for each ticker\n",
       "stats_dict = {}\n",
       "\n",
       "for ticker in tickers:\n",
       "    returns = returns_dict[ticker]\n",
       "    \n",
       "    stats_dict[ticker] = {\n",
       "        'Mean': returns.mean(),\n",
       "        'Median': returns.median(),\n",
       "        'Min': returns.min(),\n",
       "        'Max': returns.max(),\n",
       "        'Std Dev': returns.std(),\n",
       "        'Skewness': returns.skew(),\n",
       "        'Kurtosis': returns.kurtosis(),\n",
       "        'Sharpe Ratio (Annualized)': returns.mean() / returns.std() * np.sqrt(252),\n",
       "        'Positive Days %': (returns > 0).mean() * 100,\n",
       "        'Negative Days %': (returns < 0).mean() * 100\n",
       "    }\n",
       "\n",
       "# Convert to DataFrame for display\n",
       "stats_df = pd.DataFrame(stats_dict).T\n",
       "stats_df"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Return Distribution Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot return distributions\n",
       "plt.figure(figsize=(14, 6 * len(tickers)))\n",
       "\n",
       "for i, ticker in enumerate(tickers, 1):\n",
       "    returns = returns_dict[ticker]\n",
       "    \n",
       "    plt.subplot(len(tickers), 2, 2*i-1)\n",
       "    sns.histplot(returns, kde=True, stat=\"density\")\n",
       "    \n",
       "    # Add normal distribution curve\n",
       "    x = np.linspace(returns.min(), returns.max(), 100)\n",
       "    plt.plot(x, stats.norm.pdf(x, returns.mean(), returns.std()), 'r--', linewidth=2)\n",
       "    \n",
       "    plt.title(f'{ticker} Return Distribution')\n",
       "    plt.xlabel('Return')\n",
       "    plt.ylabel('Density')\n",
       "    plt.grid(True)\n",
       "    \n",
       "    # QQ plot to check normality\n",
       "    plt.subplot(len(tickers), 2, 2*i)\n",
       "    stats.probplot(returns, dist=\"norm\", plot=plt)\n",
       "    plt.title(f'{ticker} Q-Q Plot')\n",
       "    plt.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Volatility Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate rolling volatility (21-day window = approximately 1 month)\n",
       "rolling_vol_dict = {}\n",
       "for ticker in tickers:\n",
       "    rolling_vol_dict[ticker] = returns_dict[ticker].rolling(window=21).std() * np.sqrt(252)\n",
       "    \n",
       "# Plot rolling volatility\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "for ticker in tickers:\n",
       "    rolling_vol_dict[ticker].plot(label=ticker)\n",
       "    \n",
       "plt.title('Annualized Rolling Volatility (21-day window)')\n",
       "plt.xlabel('Date')\n",
       "plt.ylabel('Volatility')\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()\n",
       "\n",
       "# Volatility clustering analysis\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "for ticker in tickers:\n",
       "    # Calculate absolute returns as a measure of volatility\n",
       "    abs_returns = np.abs(returns_dict[ticker])\n",
       "    # Calculate autocorrelation of absolute returns\n",
       "    acf_values = acf(abs_returns, nlags=20, fft=True)\n",
       "    plt.plot(range(len(acf_values)), acf_values, marker='o', label=ticker)\n",
       "    \n",
       "plt.title('Autocorrelation of Absolute Returns (Volatility Clustering)')\n",
       "plt.xlabel('Lag')\n",
       "plt.ylabel('Autocorrelation')\n",
       "plt.axhline(y=0, color='r', linestyle='--')\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()\n",
       "\n",
       "# Monthly volatility\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "for ticker in tickers:\n",
       "    monthly_vol = returns_dict[ticker].groupby(returns_dict[ticker].index.month).std() * np.sqrt(252)\n",
       "    plt.bar(monthly_vol.index, monthly_vol.values, alpha=0.5, label=ticker)\n",
       "    \n",
       "plt.title('Monthly Volatility Pattern')\n",
       "plt.xlabel('Month')\n",
       "plt.ylabel('Annualized Volatility')\n",
       "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Autocorrelation Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Check for autocorrelation in returns (momentum/mean reversion effects)\n",
       "plt.figure(figsize=(14, 10))\n",
       "\n",
       "for i, ticker in enumerate(tickers, 1):\n",
       "    # Autocorrelation function\n",
       "    plt.subplot(len(tickers), 2, 2*i-1)\n",
       "    plot_acf(returns_dict[ticker], lags=20, title=f'{ticker} Return Autocorrelation')\n",
       "    plt.grid(True)\n",
       "    \n",
       "    # Partial autocorrelation function\n",
       "    plt.subplot(len(tickers), 2, 2*i)\n",
       "    plot_pacf(returns_dict[ticker], lags=20, title=f'{ticker} Return Partial Autocorrelation')\n",
       "    plt.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Stationarity Test"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Perform Augmented Dickey-Fuller test to check for stationarity\n",
       "adf_results = {}\n",
       "\n",
       "for ticker in tickers:\n",
       "    # Test on price series (expected to be non-stationary)\n",
       "    price_result = adfuller(data_dict[ticker]['Price'])\n",
       "    \n",
       "    # Test on return series (expected to be stationary)\n",
       "    returns_result = adfuller(returns_dict[ticker].dropna())\n",
       "    \n",
       "    adf_results[ticker] = {\n",
       "        'Price Series': {\n",
       "            'ADF Statistic': price_result[0],\n",
       "            'p-value': price_result[1],\n",
       "            'Critical Values': price_result[4],\n",
       "            'Stationary': price_result[1] < 0.05\n",
       "        },\n",
       "        'Return Series': {\n",
       "            'ADF Statistic': returns_result[0],\n",
       "            'p-value': returns_result[1],\n",
       "            'Critical Values': returns_result[4],\n",
       "            'Stationary': returns_result[1] < 0.05\n",
       "        }\n",
       "    }\n",
       "\n",
       "# Display results\n",
       "for ticker in tickers:\n",
       "    print(f\"\\nStationarity Test Results for {ticker}:\")\n",
       "    print(\"Price Series:\")\n",
       "    print(f\"  ADF Statistic: {adf_results[ticker]['Price Series']['ADF Statistic']:.4f}\")\n",
       "    print(f\"  p-value: {adf_results[ticker]['Price Series']['p-value']:.4f}\")\n",
       "    print(f\"  Is Stationary: {adf_results[ticker]['Price Series']['Stationary']}\")\n",
       "    \n",
       "    print(\"\\nReturn Series:\")\n",
       "    print(f\"  ADF Statistic: {adf_results[ticker]['Return Series']['ADF Statistic']:.4f}\")\n",
       "    print(f\"  p-value: {adf_results[ticker]['Return Series']['p-value']:.4f}\")\n",
       "    print(f\"  Is Stationary: {adf_results[ticker]['Return Series']['Stationary']}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Seasonality Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Analyze day-of-week effects\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "for ticker in tickers:\n",
       "    # Get average return by day of week\n",
       "    dow_returns = returns_dict[ticker].groupby(returns_dict[ticker].index.dayofweek).mean() * 100\n",
       "    \n",
       "    # Reindex to make Monday first\n",
       "    dow_returns.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
       "    \n",
       "    # Plot\n",
       "    plt.bar(dow_returns.index, dow_returns.values, alpha=0.5, label=ticker)\n",
       "\n",
       "plt.title('Average Daily Returns by Day of Week')\n",
       "plt.xlabel('Day of Week')\n",
       "plt.ylabel('Average Return (%)')\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()\n",
       "\n",
       "# Analyze month-of-year effects\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "for ticker in tickers:\n",
       "    # Get average return by month\n",
       "    monthly_returns = returns_dict[ticker].groupby(returns_dict[ticker].index.month).mean() * 100 * 21  # Approximate trading days per month\n",
       "    \n",
       "    # Reindex to use month names\n",
       "    monthly_returns.index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
       "    \n",
       "    # Plot\n",
       "    plt.bar(monthly_returns.index, monthly_returns.values, alpha=0.5, label=ticker)\n",
       "\n",
       "plt.title('Average Monthly Returns by Month')\n",
       "plt.xlabel('Month')\n",
       "plt.ylabel('Average Monthly Return (%)')\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Correlation Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a DataFrame with returns from all tickers\n",
       "combined_returns = pd.DataFrame()\n",
       "for ticker in tickers:\n",
       "    combined_returns[ticker] = returns_dict[ticker]\n",
       "\n",
       "# Calculate correlation matrix\n",
       "correlation_matrix = combined_returns.corr()\n",
       "\n",
       "# Plot correlation matrix as a heatmap\n",
       "plt.figure(figsize=(10, 8))\n",
       "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
       "plt.title('Return Correlation Matrix')\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Calculate rolling correlation (if we have multiple tickers)\n",
       "if len(tickers) > 1:\n",
       "    plt.figure(figsize=(14, 7))\n",
       "    \n",
       "    # Calculate 60-day rolling correlation between the first two tickers\n",
       "    rolling_corr = combined_returns[tickers[0]].rolling(window=60).corr(combined_returns[tickers[1]])\n",
       "    \n",
       "    plt.plot(rolling_corr.index, rolling_corr.values)\n",
       "    plt.title(f'60-day Rolling Correlation between {tickers[0]} and {tickers[1]}')\n",
       "    plt.xlabel('Date')\n",
       "    plt.ylabel('Correlation')\n",
       "    plt.axhline(y=correlation_matrix.loc[tickers[0], tickers[1]], color='r', linestyle='--', label='Full-period Correlation')\n",
       "    plt.legend()\n",
       "    plt.grid(True)\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Risk-Reward Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate annualized return and risk metrics\n",
       "annualized_metrics = {}\n",
       "\n",
       "for ticker in tickers:\n",
       "    returns = returns_dict[ticker]\n",
       "    \n",
       "    # Calculate annualized metrics\n",
       "    annualized_return = returns.mean() * 252\n",
       "    annualized_vol = returns.std() * np.sqrt(252)\n",
       "    sharpe_ratio = annualized_return / annualized_vol\n",
       "    \n",
       "    # Calculate downside risk metrics\n",
       "    downside_returns = returns[returns < 0]\n",
       "    downside_vol = downside_returns.std() * np.sqrt(252)\n",
       "    sortino_ratio = annualized_return / downside_vol if downside_vol != 0 else np.nan\n",
       "    \n",
       "    # Calculate maximum drawdown\n",
       "    cumulative_returns = (1 + returns).cumprod()\n",
       "    running_max = cumulative_returns.cummax()\n",
       "    drawdown = (cumulative_returns / running_max - 1)\n",
       "    max_drawdown = drawdown.min()\n",
       "    \n",
       "    annualized_metrics[ticker] = {\n",
       "        'Annualized Return': annualized_return,\n",
       "        'Annualized Volatility': annualized_vol,\n",
       "        'Sharpe Ratio': sharpe_ratio,\n",
       "        'Downside Volatility': downside_vol,\n",
       "        'Sortino Ratio': sortino_ratio,\n",
       "        'Max Drawdown': max_drawdown,\n",
       "        'Calmar Ratio': annualized_return / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
       "    }\n",
       "\n",
       "# Create DataFrame with annualized metrics\n",
       "annualized_df = pd.DataFrame(annualized_metrics).T\n",
       "\n",
       "# Format as percentages where appropriate\n",
       "for col in ['Annualized Return', 'Annualized Volatility', 'Downside Volatility', 'Max Drawdown']:\n",
       "    annualized_df[col] = annualized_df[col] * 100\n",
       "\n",
       "# Display table\n",
       "annualized_df\n",
       "\n",
       "# Plot risk vs. return\n",
       "plt.figure(figsize=(12, 8))\n",
       "for ticker in tickers:\n",
       "    plt.scatter(\n",
       "        annualized_metrics[ticker]['Annualized Volatility'], \n",
       "        annualized_metrics[ticker]['Annualized Return'],\n",
       "        s=100, label=ticker\n",
       "    )\n",
       "    plt.annotate(\n",
       "        ticker, \n",
       "        xy=(annualized_metrics[ticker]['Annualized Volatility'], annualized_metrics[ticker]['Annualized Return']),\n",
       "        xytext=(5, 5), textcoords='offset points'\n",
       "    )\n",
       "\n",
       "plt.title('Risk-Return Profile')\n",
       "plt.xlabel('Annualized Volatility (%)')\n",
       "plt.ylabel('Annualized Return (%)')\n",
       "plt.grid(True)\n",
       "plt.axhline(y=0, color='r', linestyle='--')\n",
       "\n",
       "# Add Sharpe ratio lines\n",
       "x = np.linspace(0, max([annualized_metrics[ticker]['Annualized Volatility'] for ticker in tickers]) * 1.5, 100)\n",
       "for sharpe in [0.5, 1, 1.5, 2]:\n",
       "    y = sharpe * x\n",
       "    plt.plot(x, y, 'k--', alpha=0.3)\n",
       "    plt.annotate(f'Sharpe = {sharpe}', xy=(x[-1], y[-1]), xytext=(5, 0), textcoords='offset points', alpha=0.5)\n",
       "\n",
       "plt.legend()\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion and Trading Strategy Implications"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Based on our analysis, we can draw several conclusions about the market characteristics:\n",
       "\n",
       "markdown_text = \"\"\"\n",
       "### Key Findings\n",
       "\n",
       "1. **Stationarity**:\n",
       "   - As expected, price series are non-stationary (random walk behavior)\n",
       "   - Return series are stationary, allowing for predictive modeling\n",
       "\n",
       "2. **Volatility Characteristics**:\n",
       "   - Evidence of volatility clustering (periods of high/low volatility tend to persist)\n",
       "   - Seasonal patterns in volatility (monthly/yearly)\n",
       "   \n",
       "3. **Return Patterns**:\n",
       "   - Day-of-week effects: [Summarize any patterns observed]\n",
       "   - Month-of-year effects: [Summarize any patterns observed]\n",
       "   \n",
       "4. **Autocorrelation**:\n",
       "   - Short-term: [Summarize findings - presence of momentum or mean-reversion]\n",
       "   - Long-term: [Summarize findings]\n",
       "\n",
       "### Trading Strategy Implications\n",
       "\n",
       "1. **Momentum Strategy Potential**:\n",
       "   - Positive autocorrelation at certain lags suggests momentum strategies may be effective\n",
       "   - Optimal lookback periods appear to be [X] days based on autocorrelation\n",
       "   \n",
       "2. **Mean Reversion Strategy Potential**:\n",
       "   - Negative autocorrelation at certain lags suggests mean-reversion strategies may be effective\n",
       "   - Z-score calculation with [Y] day window might be effective\n",
       "\n",
       "3. **Volatility-Based Strategies**:\n",
       "   - Position sizing should account for volatility clustering\n",
       "   - Consider volatility breakout strategies during transitions between regimes\n",
       "   \n",
       "4. **Seasonal Strategies**:\n",
       "   - Consider monthly rotation strategies based on observed patterns\n",
       "   - Day-of-week effects can inform optimal trade entry timing\n",
       "\n",
       "5. **Risk Management**:\n",
       "   - Non-normal return distributions suggest higher risk of extreme events than standard models predict\n",
       "   - Fat tails observed in return distributions require robust risk measures (like VaR)\n",
       "   - Drawdown control is essential given the observed maximum drawdowns\n",
       "\n",
       "### Next Steps\n",
       "\n",
       "1. Implement these findings in strategy development\n",
       "2. Test various lookback periods for momentum/mean-reversion based on autocorrelation findings\n",
       "3. Incorporate volatility forecasting into position sizing algorithms\n",
       "4. Develop ensemble approach combining multiple strategy types to exploit different market regimes\n",
       "\"\"\"\n",
       "\n",
       "from IPython.display import Markdown\n",
       "display(Markdown(markdown_text))\n",
       "\n",
       "# Save key metrics for use in other notebooks\n",
       "import pickle\n",
       "import os\n",
       "\n",
       "# Create data directory if it doesn't exist\n",
       "os.makedirs('../data', exist_ok=True)\n",
       "\n",
       "metrics_data = {\n",
       "    'returns_dict': returns_dict,\n",
       "    'annualized_metrics': annualized_metrics,\n",
       "    'correlation_matrix': correlation_matrix,\n",
       "    'adf_results': adf_results\n",
       "}\n",
       "\n",
       "with open('../data/exploratory_metrics.pkl', 'wb') as f:\n",
       "    pickle.dump(metrics_data, f)\n",
       "\n",
       "print(\"Analysis complete! Key metrics saved to '../data/exploratory_metrics.pkl'\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Additional Analysis: Market Regimes"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
